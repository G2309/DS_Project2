{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"sourceType":"competition"},{"sourceId":4264054,"sourceType":"datasetVersion","datasetId":2406209}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python -V\n!pip -V\n!python -c \"import sys, pkgutil; print('numpy', pkgutil.find_loader('numpy') is not None); print('torch', pkgutil.find_loader('torch') is not None)\"\n!pip install --upgrade --no-deps timm pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg\n!pip install --upgrade --no-deps pylibjpeg==2.1.0 pylibjpeg-libjpeg==2.3.0 pylibjpeg-openjpeg==2.5.0 || true\n!pip check || true","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T04:44:00.226029Z","iopub.execute_input":"2025-10-23T04:44:00.226332Z","iopub.status.idle":"2025-10-23T04:44:11.214541Z","shell.execute_reply.started":"2025-10-23T04:44:00.226307Z","shell.execute_reply":"2025-10-23T04:44:11.213630Z"}},"outputs":[{"name":"stdout","text":"Python 3.11.13\npip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\nnumpy True\ntorch True\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\nCollecting timm\n  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pylibjpeg\n  Downloading pylibjpeg-2.1.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting pylibjpeg-libjpeg\n  Downloading pylibjpeg_libjpeg-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting pylibjpeg-openjpeg\n  Downloading pylibjpeg_openjpeg-2.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.8 kB)\nDownloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pylibjpeg-2.1.0-py3-none-any.whl (25 kB)\nDownloading pylibjpeg_libjpeg-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hDownloading pylibjpeg_openjpeg-2.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: timm, pylibjpeg-openjpeg, pylibjpeg-libjpeg, pylibjpeg\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.19\n    Uninstalling timm-1.0.19:\n      Successfully uninstalled timm-1.0.19\nSuccessfully installed pylibjpeg-2.1.0 pylibjpeg-libjpeg-2.3.0 pylibjpeg-openjpeg-2.5.0 timm-1.0.20\nRequirement already satisfied: pylibjpeg==2.1.0 in /usr/local/lib/python3.11/dist-packages (2.1.0)\nRequirement already satisfied: pylibjpeg-libjpeg==2.3.0 in /usr/local/lib/python3.11/dist-packages (2.3.0)\nRequirement already satisfied: pylibjpeg-openjpeg==2.5.0 in /usr/local/lib/python3.11/dist-packages (2.5.0)\nbigframes 2.12.0 requires google-cloud-bigquery-storage, which is not installed.\npylibjpeg-libjpeg 2.3.0 has requirement numpy<3.0,>=2.0, but you have numpy 1.26.4.\npylibjpeg-openjpeg 2.5.0 has requirement numpy<3.0,>=2.0, but you have numpy 1.26.4.\ngensim 4.3.3 has requirement scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3.\ndatasets 4.1.1 has requirement pyarrow>=21.0.0, but you have pyarrow 19.0.1.\nonnx 1.18.0 has requirement protobuf>=4.25.1, but you have protobuf 3.20.3.\ngoogle-cloud-bigtable 2.32.0 has requirement google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1.\npreprocessing 0.1.13 has requirement nltk==3.2.4, but you have nltk 3.9.1.\ncesium 0.12.4 has requirement numpy<3.0,>=2.0, but you have numpy 1.26.4.\ngoogle-colab 1.0.0 has requirement google-auth==2.38.0, but you have google-auth 2.40.3.\ngoogle-colab 1.0.0 has requirement notebook==6.5.7, but you have notebook 6.5.4.\ngoogle-colab 1.0.0 has requirement pandas==2.2.2, but you have pandas 2.2.3.\ngoogle-colab 1.0.0 has requirement requests==2.32.3, but you have requests 2.32.5.\ngoogle-colab 1.0.0 has requirement tornado==6.4.2, but you have tornado 6.5.2.\ndopamine-rl 4.1.2 has requirement gymnasium>=1.0.0, but you have gymnasium 0.29.0.\nbigframes 2.12.0 has requirement google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0.\nbigframes 2.12.0 has requirement rich<14,>=12.4.4, but you have rich 14.1.0.\nibis-framework 9.5.0 has requirement toolz<1,>=0.11, but you have toolz 1.0.0.\ntokenizers 0.21.2 has requirement huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.0rc2.\nthinc 8.3.6 has requirement numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4.\nopencv-contrib-python 4.12.0.88 has requirement numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4.\nlibcugraph-cu12 25.6.0 has requirement libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0.\nopencv-python 4.12.0.88 has requirement numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4.\ntorch 2.6.0+cu124 has requirement nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2.\ntorch 2.6.0+cu124 has requirement nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82.\ntorch 2.6.0+cu124 has requirement nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82.\ntorch 2.6.0+cu124 has requirement nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82.\ntorch 2.6.0+cu124 has requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75.\ntorch 2.6.0+cu124 has requirement nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61.\ntorch 2.6.0+cu124 has requirement nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82.\ntorch 2.6.0+cu124 has requirement nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83.\ntorch 2.6.0+cu124 has requirement nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3.\ntorch 2.6.0+cu124 has requirement nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82.\ngradio 5.38.1 has requirement pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1.\ncudf-polars-cu12 25.6.0 has requirement pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2.\nmdit-py-plugins 0.4.2 has requirement markdown-it-py<4.0.0,>=1.0.0, but you have markdown-it-py 4.0.0.\ntensorflow-metadata 1.17.2 has requirement protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3.\npydrive2 1.21.3 has requirement cryptography<44, but you have cryptography 46.0.1.\npydrive2 1.21.3 has requirement pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0.\nimbalanced-learn 0.13.0 has requirement scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2.\npandas-gbq 0.29.2 has requirement google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1.\ngoogle-cloud-storage 2.19.0 has requirement google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1.\ntransformers 4.53.3 has requirement huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2.\nopencv-python-headless 4.12.0.88 has requirement numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4.\nplotnine 0.14.5 has requirement matplotlib>=3.8.0, but you have matplotlib 3.7.2.\npylibcugraph-cu12 25.6.0 has requirement pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0.\npylibcugraph-cu12 25.6.0 has requirement rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0.\njupyter-kernel-gateway 2.5.2 has requirement jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3.\numap-learn 0.5.9.post2 has requirement scikit-learn>=1.6, but you have scikit-learn 1.2.2.\ndataproc-spark-connect 0.8.3 has requirement google-api-core>=2.19, but you have google-api-core 1.34.1.\ngcsfs 2025.3.0 has requirement fsspec==2025.3.0, but you have fsspec 2025.9.0.\nmlxtend 0.23.4 has requirement scikit-learn>=1.3.1, but you have scikit-learn 1.2.2.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, sys\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-23T04:44:12.625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport json\nfrom PIL import Image\nimport pydicom\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nimport timm\nfrom tqdm import tqdm\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T05:15:19.988771Z","iopub.execute_input":"2025-10-23T05:15:19.989478Z","iopub.status.idle":"2025-10-23T05:15:19.994197Z","shell.execute_reply.started":"2025-10-23T05:15:19.989453Z","shell.execute_reply":"2025-10-23T05:15:19.993404Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/rsna-2022-cervical-spine-fracture-detection\"\ntrain_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\nsample_sub = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\nTRAIN_IMG_DIR = os.path.join(DATA_PATH, \"train_images\")\n\nstudy_ids = train_df[\"StudyInstanceUID\"].unique()\nnp.random.seed(33)\nnp.random.shuffle(study_ids)\nsplit_idx = int(len(study_ids) * 0.8)\ntrain_studies = study_ids[:split_idx]\nval_studies = study_ids[split_idx:]\ntrain_df_split = train_df[train_df[\"StudyInstanceUID\"].isin(train_studies)]\nval_df_split = train_df[train_df[\"StudyInstanceUID\"].isin(val_studies)]","metadata":{"execution":{"iopub.status.busy":"2025-10-23T05:14:30.724129Z","iopub.execute_input":"2025-10-23T05:14:30.724405Z","iopub.status.idle":"2025-10-23T05:14:30.742793Z","shell.execute_reply.started":"2025-10-23T05:14:30.724385Z","shell.execute_reply":"2025-10-23T05:14:30.741941Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## ViT + Cervical Dataset Classes","metadata":{}},{"cell_type":"code","source":"class CervicalSliceDataset(Dataset):\n    def __init__(self, df, root, transform=None, num_slices=5):\n        self.df = df\n        self.root = root\n        self.transform = transform\n        self.num_slices = num_slices\n        self.study_ids = df[\"StudyInstanceUID\"].unique().tolist()\n    \n    def __len__(self):\n        return len(self.study_ids)\n    \n    def __getitem__(self, idx):\n        study = self.study_ids[idx]\n        folder = os.path.join(self.root, study)\n        files = sorted([f for f in os.listdir(folder) if f.endswith(\".dcm\")])\n        \n        if len(files) == 0:\n            raise RuntimeError(f\"No DICOM in {folder}\")\n        \n        indices = np.linspace(0, len(files)-1, self.num_slices, dtype=int)\n        slices = []\n        \n        for i in indices:\n            path = os.path.join(folder, files[i])\n            ds = pydicom.dcmread(path)\n            try:\n                arr = ds.pixel_array\n            except Exception:\n                ds.decompress()\n                arr = ds.pixel_array\n            \n            if arr.ndim == 3:\n                arr = arr[0]\n            \n            img = Image.fromarray(arr).convert(\"L\")\n            if self.transform:\n                img = self.transform(img)\n            slices.append(img)\n        \n        img_tensor = torch.mean(torch.stack(slices), dim=0)\n        \n        row = self.df[self.df[\"StudyInstanceUID\"]==study].iloc[0]\n        labels = torch.zeros(8, dtype=torch.float32)\n        labels[0] = row[\"patient_overall\"]\n        for i in range(1,8):\n            labels[i] = row[f\"C{i}\"]\n        \n        return img_tensor, labels\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.dropout(x)\n        return x\n\nclass MLP(nn.Module):\n    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=8,\n                 embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        num_patches = self.patch_embed.n_patches\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.dropout(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        x = self.norm(x)\n        cls_output = x[:, 0]\n        x = self.head(cls_output)\n        return x\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomRotation(15),\n    transforms.RandomHorizontalFlip(0.3),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\ntrain_ds = CervicalSliceDataset(train_df_split, TRAIN_IMG_DIR, transform=train_transforms, num_slices=5)\nval_ds = CervicalSliceDataset(val_df_split, TRAIN_IMG_DIR, transform=val_transforms, num_slices=5)\n\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T05:14:32.975721Z","iopub.execute_input":"2025-10-23T05:14:32.975988Z","iopub.status.idle":"2025-10-23T05:14:33.001487Z","shell.execute_reply.started":"2025-10-23T05:14:32.975968Z","shell.execute_reply":"2025-10-23T05:14:33.000844Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Modelo ViT","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = VisionTransformer(\n    img_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=8,\n    embed_dim=768,\n    depth=12,\n    num_heads=12,\n    mlp_ratio=4.0,\n    dropout=0.1\n)\nmodel = model.to(device)\n\npos_counts = train_df_split.iloc[:, 1:9].sum()\nneg_counts = len(train_df_split) - pos_counts\npos_weight = (neg_counts / pos_counts).values\npos_weight = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T05:14:37.881952Z","iopub.execute_input":"2025-10-23T05:14:37.882236Z","iopub.status.idle":"2025-10-23T05:14:39.336801Z","shell.execute_reply.started":"2025-10-23T05:14:37.882216Z","shell.execute_reply":"2025-10-23T05:14:39.336188Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else None\n\ndef train_epoch(model, loader, optimizer, criterion, device, scaler=None):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for imgs, labels in tqdm(loader, desc=\"Training\"):\n        imgs = imgs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        optimizer.zero_grad()\n        \n        if scaler is not None:\n            with torch.amp.autocast('cuda'):\n                out = model(imgs)\n                loss = criterion(out, labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            out = model(imgs)\n            loss = criterion(out, labels)\n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item() * imgs.size(0)\n        preds = (torch.sigmoid(out) > 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.numel()\n\n        del imgs, labels, out, loss\n        if device.type == 'cuda':\n            torch.cuda.empty_cache()\n    \n    return running_loss / len(loader.dataset), correct / total\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for imgs, labels in tqdm(loader, desc=\"Validation\"):\n            imgs = imgs.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            if torch.cuda.is_available():\n                with torch.amp.autocast('cuda'):\n                    out = model(imgs)\n                    loss = criterion(out, labels)\n            else:\n                out = model(imgs)\n                loss = criterion(out, labels)\n            \n            running_loss += loss.item() * imgs.size(0)\n            preds = (torch.sigmoid(out) > 0.5).float()\n            correct += (preds == labels).sum().item()\n            total += labels.numel()\n    \n    return running_loss / len(loader.dataset), correct / total\n\nresults = []\nbest_val_acc = 0.0\n\nfor epoch in range(1, 16):\n    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    scheduler.step()\n    \n    results.append({\n        'epoch': epoch,\n        'train_loss': train_loss,\n        'train_accuracy': train_acc,\n        'val_loss': val_loss,\n        'val_accuracy': val_acc,\n        'lr': optimizer.param_groups[0]['lr']\n    })\n    \n    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, lr={optimizer.param_groups[0]['lr']:.2e}\")\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_vit_custom.pth')\n        print(f\"  -> Saved best model with val_acc={val_acc:.4f}\")\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('training_results_custom_vit.csv', index=False)\nprint(\"\\nResultados guardados en 'training_results_custom_vit.csv'\")\nprint(f\"\\nMejor validación accuracy: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T05:19:37.696042Z","iopub.execute_input":"2025-10-23T05:19:37.696402Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 101/101 [00:48<00:00,  2.07it/s]\nValidation: 100%|██████████| 26/26 [00:09<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: train_loss=1.2418, train_acc=0.4803, val_loss=1.2577, val_acc=0.2840, lr=2.97e-04\n  -> Saved best model with val_acc=0.2840\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|████████▏ | 83/101 [00:40<00:08,  2.19it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}